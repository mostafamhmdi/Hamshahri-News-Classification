{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1e2e49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c8ea251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>PP_Text_2</th>\n",
       "      <th>length</th>\n",
       "      <th>num_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nاز سوي كميسيون ماده 10 \\nاحزابكارگزاران سازن...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>کمیسیون ماده ۱۰ احزابکارگزاران سازندگی مجوز فع...</td>\n",
       "      <td>344</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nخصوصي سازي كارخانه هاي چاي سال آينده \\nآغاز ...</td>\n",
       "      <td>Economy</td>\n",
       "      <td>خصوصی کارخانه چای آینده آغاز مدیرعامل سازمان چ...</td>\n",
       "      <td>687</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nتوضيح يك مطلب \\nسوم بهمن ماه امسال گزارش طرح...</td>\n",
       "      <td>Science and Culture</td>\n",
       "      <td>توضیح مطلب بهمن ماه امسال گزارش طرح بررسی شناس...</td>\n",
       "      <td>1120</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nبهاي گازوئيل و بنزين در \\nفرانسه افزايش يافت...</td>\n",
       "      <td>Economy</td>\n",
       "      <td>بهای گازوئیل بنزین فرانسه افزایش پاریس خبرگزار...</td>\n",
       "      <td>396</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nرئيس سازمان حج و زيارت اعلام كرد \\n شهريور 1...</td>\n",
       "      <td>Science and Culture</td>\n",
       "      <td>رئیس سازمان حج زیارت اعلام شهریور ۱۶ زمان ثبت ...</td>\n",
       "      <td>495</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\nهمزمان با روز جهاني موزه و هفته ميراث \\nجديد...</td>\n",
       "      <td>Science and Culture</td>\n",
       "      <td>همزمان روز جهانی موزه هفته میراث کشور افتتاح ف...</td>\n",
       "      <td>838</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\nنگراني درلوشامپيونا \\nسهيلا قاسمي \\nبازار نق...</td>\n",
       "      <td>Sport</td>\n",
       "      <td>نگران درلوشامپیونا سهیلا قاسم بازار نقل انتقال...</td>\n",
       "      <td>3010</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\nرئيس كل بانك مركزي: مدعيان چاپ اسكناس \\nبدون...</td>\n",
       "      <td>Economy</td>\n",
       "      <td>رئیس بانک مرکزی مدعیان چاپ اسکناس پشتوانه نگرا...</td>\n",
       "      <td>1182</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text                class  \\\n",
       "0  \\nاز سوي كميسيون ماده 10 \\nاحزابكارگزاران سازن...             Politics   \n",
       "1  \\nخصوصي سازي كارخانه هاي چاي سال آينده \\nآغاز ...              Economy   \n",
       "2  \\nتوضيح يك مطلب \\nسوم بهمن ماه امسال گزارش طرح...  Science and Culture   \n",
       "3  \\nبهاي گازوئيل و بنزين در \\nفرانسه افزايش يافت...              Economy   \n",
       "4  \\nرئيس سازمان حج و زيارت اعلام كرد \\n شهريور 1...  Science and Culture   \n",
       "5  \\nهمزمان با روز جهاني موزه و هفته ميراث \\nجديد...  Science and Culture   \n",
       "6  \\nنگراني درلوشامپيونا \\nسهيلا قاسمي \\nبازار نق...                Sport   \n",
       "7  \\nرئيس كل بانك مركزي: مدعيان چاپ اسكناس \\nبدون...              Economy   \n",
       "\n",
       "                                           PP_Text_2  length  num_tokens  \n",
       "0  کمیسیون ماده ۱۰ احزابکارگزاران سازندگی مجوز فع...     344          56  \n",
       "1  خصوصی کارخانه چای آینده آغاز مدیرعامل سازمان چ...     687         113  \n",
       "2  توضیح مطلب بهمن ماه امسال گزارش طرح بررسی شناس...    1120         196  \n",
       "3  بهای گازوئیل بنزین فرانسه افزایش پاریس خبرگزار...     396          70  \n",
       "4  رئیس سازمان حج زیارت اعلام شهریور ۱۶ زمان ثبت ...     495          95  \n",
       "5  همزمان روز جهانی موزه هفته میراث کشور افتتاح ف...     838         150  \n",
       "6  نگران درلوشامپیونا سهیلا قاسم بازار نقل انتقال...    3010         510  \n",
       "7  رئیس بانک مرکزی مدعیان چاپ اسکناس پشتوانه نگرا...    1182         201  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('final_train.csv')\n",
    "df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7ee03ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b89924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "tokenized_texts = [tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\") for text in df['PP_Text_2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ceef8cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_ids = torch.cat([text['input_ids'] for text in tokenized_texts], dim=0)\n",
    "attention_mask = torch.cat([text['attention_mask'] for text in tokenized_texts], dim=0)\n",
    "\n",
    "# Encode labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(df['class'])\n",
    "\n",
    "# Create DataLoader objects\n",
    "batch_size = 32\n",
    "train_inputs, test_inputs, train_masks, test_masks, train_labels, test_labels = train_test_split(input_ids, attention_mask, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff96a192",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(train_inputs, train_masks, torch.tensor(train_labels, dtype=torch.long))\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, torch.tensor(test_labels, dtype=torch.long))\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a534e144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c526206389fb4cb7911d1b88054e1b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                         | 0/1500 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                              | 1/1500 [00:51<21:15:28, 51.05s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                              | 2/1500 [01:13<14:09:08, 34.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|▏                                                                             | 3/1500 [01:35<11:58:37, 28.80s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|▏                                                                             | 4/1500 [01:57<10:49:49, 26.06s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|▎                                                                              | 5/1500 [02:17<9:56:39, 23.95s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|▎                                                                              | 6/1500 [02:37<9:23:15, 22.62s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|▎                                                                              | 7/1500 [02:57<9:02:39, 21.81s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  1%|▍                                                                             | 8/1500 [03:37<11:15:01, 27.15s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [18], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[0;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m---> 16\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=len(label_encoder.classes_))  # Adjust num_labels\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6b8db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "\n",
    "for batch in tqdm(test_dataloader):\n",
    "    input_ids = batch[0].to(device)\n",
    "    attention_mask = batch[1].to(device)\n",
    "    labels = batch[2].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    test_preds.extend(logits.argmax(dim=1).tolist())\n",
    "    test_labels.extend(labels.tolist())\n",
    "\n",
    "# Calculate and print classification report\n",
    "class_names = [\"class1\", \"class2\", \"class3\", \"class4\", \"class5\"]  # Replace with your actual class names\n",
    "report = classification_report(test_labels, test_preds, target_names=class_names)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
